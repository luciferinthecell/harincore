"""
í•˜ë¦°ì½”ì–´ ë©”ëª¨ë¦¬ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
===========================================

ê¸°ì¡´ íŒŒì¼ë“¤ì„ ìƒˆë¡œìš´ ì €ì¥ êµ¬ì¡°ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ê³  ë§¤í•‘
"""

import json
import shutil
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib


class MemoryFileMigration:
    """ë©”ëª¨ë¦¬ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.base_path = Path("memory/data")
        self.old_data_path = Path("data/memory_data")
        self.old_intent_cache_path = Path("data/intent_cache")
        self.old_test_intent_cache_path = Path("data/test_intent_cache")
        
        # ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ êµ¬ì¡°
        self.new_structure = {
            "integrated": {
                "hot_memory.jsonl": [],
                "warm_memory.jsonl": [],
                "cold_memory.jsonl": [],
                "contextual_memory.jsonl": [],
                "vector_memory.jsonl": []
            },
            "palantir": {
                "main_graph.json": None,
                "multiverse_graph.json": None,
                "relationships.jsonl": []
            },
            "conductor": {
                "entities.jsonl": [],
                "relationships.jsonl": [],
                "contradictions.jsonl": [],
                "narratives.jsonl": []
            },
            "data_memory": {
                "h1.jsonl": [],
                "h2.jsonl": [],
                "h3.jsonl": [],
                "ha1.jsonl": [],
                "h.json": None
            },
            "cache": {
                "thought_cache.jsonl": [],
                "memory_cache.jsonl": [],
                "loop_cache.jsonl": [],
                "emotional_cache.jsonl": [],
                "cache_manager.json": None
            },
            "intent_cache": {
                "intent_analysis_*.json": [],
                "intent_cache_manager.json": None
            },
            "legacy": {
                "old_memory_data/": [],
                "old_intent_cache/": [],
                "migration_log.json": None
            }
        }
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸
        self.migration_log = {
            "migration_date": datetime.now().isoformat(),
            "files_migrated": [],
            "files_skipped": [],
            "errors": [],
            "statistics": {}
        }
    
    def analyze_existing_files(self) -> Dict[str, Any]:
        """ê¸°ì¡´ íŒŒì¼ ë¶„ì„"""
        print("ğŸ” ê¸°ì¡´ íŒŒì¼ ë¶„ì„ ì¤‘...")
        
        analysis = {
            "memory_data": {},
            "intent_cache": {},
            "palantir": {},
            "total_files": 0,
            "total_size": 0,
            "file_types": {},
            "duplicates": []
        }
        
        # memory_data ë¶„ì„
        if self.old_data_path.exists():
            for file_path in self.old_data_path.iterdir():
                if file_path.is_file():
                    file_info = self._analyze_file(file_path)
                    analysis["memory_data"][file_path.name] = file_info
                    analysis["total_files"] += 1
                    analysis["total_size"] += file_info["size_bytes"]
                    
                    # íŒŒì¼ íƒ€ì… í†µê³„
                    file_type = file_info["file_type"]
                    if file_type not in analysis["file_types"]:
                        analysis["file_types"][file_type] = 0
                    analysis["file_types"][file_type] += 1
        
        # intent_cache ë¶„ì„
        if self.old_intent_cache_path.exists():
            for file_path in self.old_intent_cache_path.iterdir():
                if file_path.is_file():
                    file_info = self._analyze_file(file_path)
                    analysis["intent_cache"][file_path.name] = file_info
                    analysis["total_files"] += 1
                    analysis["total_size"] += file_info["size_bytes"]
        
        # test_intent_cache ë¶„ì„
        if self.old_test_intent_cache_path.exists():
            for file_path in self.old_test_intent_cache_path.iterdir():
                if file_path.is_file():
                    file_info = self._analyze_file(file_path)
                    analysis["intent_cache"][file_path.name] = file_info
                    analysis["total_files"] += 1
                    analysis["total_size"] += file_info["size_bytes"]
        
        # palantir ë¶„ì„
        palantir_file = self.base_path / "palantir_graph.json"
        if palantir_file.exists():
            file_info = self._analyze_file(palantir_file)
            analysis["palantir"]["palantir_graph.json"] = file_info
            analysis["total_files"] += 1
            analysis["total_size"] += file_info["size_bytes"]
        
        # ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬
        analysis["duplicates"] = self._find_duplicates(analysis)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {analysis['total_files']}ê°œ íŒŒì¼, {analysis['total_size'] / 1024:.1f}KB")
        return analysis
    
    def _analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        stat = file_path.stat()
        
        file_info = {
            "path": str(file_path),
            "size_bytes": stat.st_size,
            "size_kb": stat.st_size / 1024,
            "last_modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "file_type": file_path.suffix,
            "content_type": self._determine_content_type(file_path),
            "memory_count": self._count_memories_in_file(file_path),
            "file_hash": self._calculate_file_hash(file_path)
        }
        
        return file_info
    
    def _determine_content_type(self, file_path: Path) -> str:
        """íŒŒì¼ ë‚´ìš© íƒ€ì… ê²°ì •"""
        try:
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        if 'nodes' in data and 'edges' in data:
                            return 'graph'
                        elif 'memories' in data:
                            return 'memory_collection'
                        elif 'cache' in data:
                            return 'cache'
                        else:
                            return 'general_json'
                    elif isinstance(data, list):
                        return 'memory_list'
                    else:
                        return 'unknown'
            elif file_path.suffix == '.jsonl':
                return 'memory_stream'
            else:
                return 'unknown'
        except Exception:
            return 'unknown'
    
    def _count_memories_in_file(self, file_path: Path) -> int:
        """íŒŒì¼ ë‚´ ë©”ëª¨ë¦¬ ê°œìˆ˜ ê³„ì‚°"""
        try:
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        if 'nodes' in data:
                            return len(data.get('nodes', []))
                        elif 'memories' in data:
                            return len(data.get('memories', []))
                        else:
                            return 1
                    elif isinstance(data, list):
                        return len(data)
                    else:
                        return 1
            elif file_path.suffix == '.jsonl':
                count = 0
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            count += 1
                return count
            else:
                return 0
        except Exception:
            return 0
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
                return hashlib.md5(content).hexdigest()
        except Exception:
            return ""
    
    def _find_duplicates(self, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°"""
        duplicates = []
        file_hashes = {}
        
        for category in ['memory_data', 'intent_cache', 'palantir']:
            for filename, file_info in analysis.get(category, {}).items():
                file_hash = file_info.get('file_hash', '')
                if file_hash:
                    if file_hash in file_hashes:
                        duplicates.append({
                            'original': file_hashes[file_hash],
                            'duplicate': {'category': category, 'filename': filename, 'info': file_info}
                        })
                    else:
                        file_hashes[file_hash] = {'category': category, 'filename': filename, 'info': file_info}
        
        return duplicates
    
    def create_migration_plan(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìƒì„±"""
        print("ğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìƒì„± ì¤‘...")
        
        plan = {
            "migration_steps": [],
            "file_mappings": {},
            "backup_plan": {},
            "cleanup_plan": {},
            "estimated_time": "5-10ë¶„"
        }
        
        # 1ë‹¨ê³„: ë°±ì—… ìƒì„±
        plan["migration_steps"].append({
            "step": 1,
            "action": "backup_existing_files",
            "description": "ê¸°ì¡´ íŒŒì¼ë“¤ ë°±ì—…",
            "files": list(analysis.get("memory_data", {}).keys()) + 
                    list(analysis.get("intent_cache", {}).keys()) +
                    list(analysis.get("palantir", {}).keys())
        })
        
        # 2ë‹¨ê³„: ìƒˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        plan["migration_steps"].append({
            "step": 2,
            "action": "create_new_structure",
            "description": "ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±",
            "directories": list(self.new_structure.keys())
        })
        
        # 3ë‹¨ê³„: íŒŒì¼ ë§¤í•‘ ë° ì´ë™
        file_mappings = self._create_file_mappings(analysis)
        plan["file_mappings"] = file_mappings
        
        for mapping in file_mappings:
            plan["migration_steps"].append({
                "step": 3,
                "action": "migrate_file",
                "description": f"íŒŒì¼ ì´ë™: {mapping['source']} â†’ {mapping['destination']}",
                "source": mapping['source'],
                "destination": mapping['destination'],
                "category": mapping['category']
            })
        
        # 4ë‹¨ê³„: ì¤‘ë³µ íŒŒì¼ ì •ë¦¬
        if analysis.get("duplicates"):
            plan["migration_steps"].append({
                "step": 4,
                "action": "cleanup_duplicates",
                "description": "ì¤‘ë³µ íŒŒì¼ ì •ë¦¬",
                "duplicates": analysis["duplicates"]
            })
        
        # 5ë‹¨ê³„: ê²€ì¦ ë° ë¡œê·¸ ìƒì„±
        plan["migration_steps"].append({
            "step": 5,
            "action": "verify_migration",
            "description": "ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ë° ë¡œê·¸ ìƒì„±"
        })
        
        print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìƒì„± ì™„ë£Œ: {len(plan['migration_steps'])}ë‹¨ê³„")
        return plan
    
    def _create_file_mappings(self, analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """íŒŒì¼ ë§¤í•‘ ìƒì„±"""
        mappings = []
        
        # memory_data íŒŒì¼ ë§¤í•‘
        memory_data_files = analysis.get("memory_data", {})
        
        # Palantir ê´€ë ¨
        if "palantir_graph.json" in memory_data_files:
            mappings.append({
                "source": "memory/data/palantir_graph.json",
                "destination": "memory/data/palantir/main_graph.json",
                "category": "palantir",
                "action": "move"
            })
        
        # MemoryConductor ê´€ë ¨
        conductor_mappings = {
            "entities.jsonl": "conductor/entities.jsonl",
            "relationships.jsonl": "conductor/relationships.jsonl", 
            "contradictions.jsonl": "conductor/contradictions.jsonl",
            "narrative_memories.jsonl": "conductor/narratives.jsonl"
        }
        
        for old_name, new_path in conductor_mappings.items():
            if old_name in memory_data_files:
                mappings.append({
                    "source": f"data/memory_data/{old_name}",
                    "destination": f"memory/data/{new_path}",
                    "category": "conductor",
                    "action": "move"
                })
        
        # DataMemory ê´€ë ¨
        data_memory_mappings = {
            "h1.jsonl": "data_memory/h1.jsonl",
            "h2.jsonl": "data_memory/h2.jsonl", 
            "h3.jsonl": "data_memory/h3.jsonl",
            "ha1.jsonl": "data_memory/ha1.jsonl",
            "h.json": "data_memory/h.json"
        }
        
        for old_name, new_path in data_memory_mappings.items():
            if old_name in memory_data_files:
                mappings.append({
                    "source": f"data/memory_data/{old_name}",
                    "destination": f"memory/data/{new_path}",
                    "category": "data_memory",
                    "action": "move"
                })
        
        # Cache ê´€ë ¨
        cache_mappings = {
            "thought_cache.jsonl": "cache/thought_cache.jsonl",
            "memory_cache.jsonl": "cache/memory_cache.jsonl",
            "loop_cache.jsonl": "cache/loop_cache.jsonl", 
            "emotional_cache.jsonl": "cache/emotional_cache.jsonl",
            "cache_manager.json": "cache/cache_manager.json"
        }
        
        for old_name, new_path in cache_mappings.items():
            if old_name in memory_data_files:
                mappings.append({
                    "source": f"data/memory_data/{old_name}",
                    "destination": f"memory/data/{new_path}",
                    "category": "cache",
                    "action": "move"
                })
        
        # Intent Cache ê´€ë ¨
        intent_cache_files = analysis.get("intent_cache", {})
        for filename in intent_cache_files:
            if filename.startswith("intent_analysis_"):
                mappings.append({
                    "source": f"data/test_intent_cache/{filename}",
                    "destination": f"memory/data/intent_cache/{filename}",
                    "category": "intent_cache",
                    "action": "move"
                })
        
        return mappings
    
    def execute_migration(self, plan: Dict[str, Any], dry_run: bool = False) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        print(f"ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ {'ì‹œë®¬ë ˆì´ì…˜' if dry_run else 'ì‹¤í–‰'} ì‹œì‘...")
        
        results = {
            "success": True,
            "migrated_files": [],
            "skipped_files": [],
            "errors": [],
            "backup_created": None,
            "execution_time": None
        }
        
        start_time = datetime.now()
        
        try:
            # 1ë‹¨ê³„: ë°±ì—… ìƒì„±
            if not dry_run:
                backup_path = self._create_backup()
                results["backup_created"] = backup_path
                print(f"ğŸ’¾ ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
            
            # 2ë‹¨ê³„: ìƒˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
            if not dry_run:
                self._create_new_directories()
                print("ğŸ“ ìƒˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")
            
            # 3ë‹¨ê³„: íŒŒì¼ ì´ë™
            for mapping in plan["file_mappings"]:
                try:
                    if not dry_run:
                        success = self._migrate_file(mapping)
                        if success:
                            results["migrated_files"].append(mapping)
                            print(f"âœ… {mapping['source']} â†’ {mapping['destination']}")
                        else:
                            results["skipped_files"].append(mapping)
                            print(f"âš ï¸ ê±´ë„ˆëœ€: {mapping['source']}")
                    else:
                        # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
                        results["migrated_files"].append(mapping)
                        print(f"ğŸ” ì‹œë®¬ë ˆì´ì…˜: {mapping['source']} â†’ {mapping['destination']}")
                        
                except Exception as e:
                    error_msg = f"íŒŒì¼ ì´ë™ ì‹¤íŒ¨ {mapping['source']}: {str(e)}"
                    results["errors"].append(error_msg)
                    print(f"âŒ {error_msg}")
            
            # 4ë‹¨ê³„: ì¤‘ë³µ íŒŒì¼ ì •ë¦¬
            if plan.get("migration_steps"):
                duplicate_step = next((step for step in plan["migration_steps"] 
                                     if step["action"] == "cleanup_duplicates"), None)
                if duplicate_step and not dry_run:
                    self._cleanup_duplicates(duplicate_step["duplicates"])
                    print("ğŸ§¹ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
            
            # 5ë‹¨ê³„: ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ìƒì„±
            if not dry_run:
                self._create_migration_log(results)
                print("ğŸ“ ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ìƒì„± ì™„ë£Œ")
            
            results["execution_time"] = (datetime.now() - start_time).total_seconds()
            
            print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ {'ì‹œë®¬ë ˆì´ì…˜' if dry_run else 'ì‹¤í–‰'} ì™„ë£Œ")
            print(f"ğŸ“Š ê²°ê³¼: {len(results['migrated_files'])}ê°œ ì´ë™, {len(results['skipped_files'])}ê°œ ê±´ë„ˆëœ€, {len(results['errors'])}ê°œ ì˜¤ë¥˜")
            
        except Exception as e:
            results["success"] = False
            results["errors"].append(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _create_backup(self) -> str:
        """ë°±ì—… ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = self.base_path / "legacy" / f"backup_{timestamp}"
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # ê¸°ì¡´ íŒŒì¼ë“¤ ë°±ì—…
        if self.old_data_path.exists():
            shutil.copytree(self.old_data_path, backup_path / "memory_data")
        
        if self.old_intent_cache_path.exists():
            shutil.copytree(self.old_intent_cache_path, backup_path / "intent_cache")
        
        if self.old_test_intent_cache_path.exists():
            shutil.copytree(self.old_test_intent_cache_path, backup_path / "test_intent_cache")
        
        # palantir íŒŒì¼ ë°±ì—…
        palantir_file = self.base_path / "palantir_graph.json"
        if palantir_file.exists():
            shutil.copy2(palantir_file, backup_path / "palantir_graph.json")
        
        return str(backup_path)
    
    def _create_new_directories(self):
        """ìƒˆ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
        for category in self.new_structure.keys():
            category_path = self.base_path / category
            category_path.mkdir(parents=True, exist_ok=True)
    
    def _migrate_file(self, mapping: Dict[str, Any]) -> bool:
        """ê°œë³„ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        source_path = Path(mapping["source"])
        dest_path = self.base_path / mapping["destination"]
        
        if not source_path.exists():
            return False
        
        # ëª©ì  ë””ë ‰í† ë¦¬ ìƒì„±
        dest_path.parent.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ì´ë™
        shutil.move(str(source_path), str(dest_path))
        
        # ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ì—…ë°ì´íŠ¸
        self.migration_log["files_migrated"].append({
            "source": mapping["source"],
            "destination": mapping["destination"],
            "category": mapping["category"],
            "timestamp": datetime.now().isoformat()
        })
        
        return True
    
    def _cleanup_duplicates(self, duplicates: List[Dict[str, Any]]):
        """ì¤‘ë³µ íŒŒì¼ ì •ë¦¬"""
        for duplicate_info in duplicates:
            duplicate = duplicate_info["duplicate"]
            category = duplicate["category"]
            filename = duplicate["filename"]
            
            if category == "memory_data":
                file_path = self.old_data_path / filename
            elif category == "intent_cache":
                file_path = self.old_intent_cache_path / filename
            else:
                continue
            
            if file_path.exists():
                # ì¤‘ë³µ íŒŒì¼ì„ legacyë¡œ ì´ë™
                legacy_path = self.base_path / "legacy" / "duplicates" / filename
                legacy_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.move(str(file_path), str(legacy_path))
                
                self.migration_log["files_skipped"].append({
                    "file": str(file_path),
                    "reason": "duplicate",
                    "timestamp": datetime.now().isoformat()
                })
    
    def _create_migration_log(self, results: Dict[str, Any]):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ìƒì„±"""
        self.migration_log.update({
            "migration_results": results,
            "statistics": {
                "total_migrated": len(results["migrated_files"]),
                "total_skipped": len(results["skipped_files"]),
                "total_errors": len(results["errors"]),
                "execution_time_seconds": results["execution_time"]
            }
        })
        
        log_path = self.base_path / "legacy" / "migration_log.json"
        with open(log_path, 'w', encoding='utf-8') as f:
            json.dump(self.migration_log, f, ensure_ascii=False, indent=2)
    
    def verify_migration(self) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦"""
        print("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì¤‘...")
        
        verification = {
            "success": True,
            "missing_files": [],
            "unexpected_files": [],
            "file_count_matches": True,
            "structure_correct": True
        }
        
        # ìƒˆ êµ¬ì¡° ê²€ì¦
        for category, files in self.new_structure.items():
            category_path = self.base_path / category
            if not category_path.exists():
                verification["structure_correct"] = False
                verification["missing_files"].append(f"ë””ë ‰í† ë¦¬ ì—†ìŒ: {category}")
                continue
            
            for filename in files.keys():
                if filename.endswith('*'):
                    # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´
                    pattern = filename.replace('*', '')
                    matching_files = list(category_path.glob(f"*{pattern}*"))
                    if not matching_files:
                        verification["missing_files"].append(f"{category}/{filename}")
                else:
                    file_path = category_path / filename
                    if not file_path.exists():
                        verification["missing_files"].append(f"{category}/{filename}")
        
        # ê¸°ì¡´ íŒŒì¼ë“¤ì´ ëª¨ë‘ ì´ë™ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if self.old_data_path.exists():
            remaining_files = list(self.old_data_path.iterdir())
            if remaining_files:
                verification["unexpected_files"].extend([str(f) for f in remaining_files])
        
        verification["success"] = (
            verification["structure_correct"] and
            len(verification["missing_files"]) == 0 and
            len(verification["unexpected_files"]) == 0
        )
        
        if verification["success"]:
            print("âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì„±ê³µ")
        else:
            print("âš ï¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€ì¦ ì‹¤íŒ¨")
            if verification["missing_files"]:
                print(f"   ëˆ„ë½ëœ íŒŒì¼: {verification['missing_files']}")
            if verification["unexpected_files"]:
                print(f"   ì˜ˆìƒì¹˜ ëª»í•œ íŒŒì¼: {verification['unexpected_files']}")
        
        return verification


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ í•˜ë¦°ì½”ì–´ ë©”ëª¨ë¦¬ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print("=" * 50)
    
    migrator = MemoryFileMigration()
    
    # 1. ê¸°ì¡´ íŒŒì¼ ë¶„ì„
    analysis = migrator.analyze_existing_files()
    
    # 2. ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìƒì„±
    plan = migrator.create_migration_plan(analysis)
    
    # 3. ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
    print("\nğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰...")
    simulation_results = migrator.execute_migration(plan, dry_run=True)
    
    # 4. ì‚¬ìš©ì í™•ì¸
    print(f"\nğŸ“Š ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
    print(f"   ì´ë™í•  íŒŒì¼: {len(simulation_results['migrated_files'])}ê°œ")
    print(f"   ê±´ë„ˆë›¸ íŒŒì¼: {len(simulation_results['skipped_files'])}ê°œ")
    print(f"   ì˜¤ë¥˜: {len(simulation_results['errors'])}ê°œ")
    
    if simulation_results['errors']:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ:")
        for error in simulation_results['errors']:
            print(f"   - {error}")
        return
    
    # 5. ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    print(f"\nğŸš€ ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ", end="")
    user_input = input().strip().lower()
    
    if user_input == 'y':
        print("\nğŸ”„ ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰...")
        results = migrator.execute_migration(plan, dry_run=False)
        
        # 6. ê²€ì¦
        verification = migrator.verify_migration()
        
        if verification["success"]:
            print("\nğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
            print("ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ì €ì¥ êµ¬ì¡°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ë§ˆì´ê·¸ë ˆì´ì…˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            print("legacy í´ë”ì˜ migration_log.jsonì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        print("\nâŒ ë§ˆì´ê·¸ë ˆì´ì…˜ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main() 