"""
í•˜ë¦°ì½”ì–´ ë©”ëª¨ë¦¬ ì €ì¥ ê´€ë¦¬ì (Memory Storage Manager)
====================================================

ëª¨ë“  ë©”ëª¨ë¦¬ íŒŒì¼ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œ

ì €ì¥ êµ¬ì¡°:
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ integrated/           # í†µí•© ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
â”‚   â”‚   â”‚   â”œâ”€â”€ hot_memory.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ warm_memory.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ cold_memory.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ contextual_memory.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ vector_memory.jsonl
â”‚   â”‚   â”œâ”€â”€ palantir/            # Palantir ì‹œìŠ¤í…œ
â”‚   â”‚   â”‚   â”œâ”€â”€ main_graph.json
â”‚   â”‚   â”‚   â”œâ”€â”€ multiverse_graph.json
â”‚   â”‚   â”‚   â””â”€â”€ relationships.jsonl
â”‚   â”‚   â”œâ”€â”€ conductor/           # MemoryConductor
â”‚   â”‚   â”‚   â”œâ”€â”€ entities.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ relationships.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ contradictions.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ narratives.jsonl
â”‚   â”‚   â”œâ”€â”€ data_memory/         # DataMemoryManager
â”‚   â”‚   â”‚   â”œâ”€â”€ h1.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ h2.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ h3.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ ha1.jsonl
â”‚   â”‚   â”œâ”€â”€ cache/               # ìºì‹œ ì‹œìŠ¤í…œ
â”‚   â”‚   â”‚   â”œâ”€â”€ thought_cache.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_cache.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ loop_cache.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ emotional_cache.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ cache_manager.json
â”‚   â”‚   â”œâ”€â”€ intent_cache/        # ì˜ë„ ë¶„ì„ ìºì‹œ
â”‚   â”‚   â”‚   â”œâ”€â”€ intent_analysis_*.json
â”‚   â”‚   â”‚   â””â”€â”€ intent_cache_manager.json
â”‚   â”‚   â””â”€â”€ legacy/              # ê¸°ì¡´ íŒŒì¼ë“¤ (ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)
â”‚   â”‚       â”œâ”€â”€ old_palantir_graph.json
â”‚   â”‚       â””â”€â”€ old_memory_data/
â”‚   â””â”€â”€ backups/                 # ë°±ì—… íŒŒì¼ë“¤
â”‚       â”œâ”€â”€ daily/
â”‚       â”œâ”€â”€ weekly/
â”‚       â””â”€â”€ monthly/
"""

import json
import shutil
import gzip
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
import uuid
import os

# ----------------------------------------------
# Optional LangChain / LangGraph integration
# ----------------------------------------------
try:
    from langchain.schema import Document
    from langchain.vectorstores import FAISS
    from langchain.embeddings.openai import OpenAIEmbeddings
    LANGCHAIN_AVAILABLE = True
except Exception:
    LANGCHAIN_AVAILABLE = False

try:
    from langgraph.graph import StateGraph, END  # type: ignore
    from langgraph.prebuilt import ToolNode  # type: ignore
    from langgraph.checkpoint.memory import MemorySaver  # type: ignore
    LANGGRAPH_AVAILABLE = True
except Exception:
    LANGGRAPH_AVAILABLE = False


@dataclass
class StorageConfig:
    """ì €ì¥ ì„¤ì •"""
    base_path: str = "memory/data"
    backup_enabled: bool = True
    compression_enabled: bool = True
    max_file_size_mb: int = 100
    auto_cleanup_days: int = 30
    backup_retention_days: int = 90


@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´"""
    path: Path
    size_bytes: int
    last_modified: datetime
    file_type: str
    memory_count: int = 0
    compression_ratio: float = 1.0


class MemoryStorageManager:
    """ë©”ëª¨ë¦¬ ì €ì¥ ê´€ë¦¬ì"""
    
    def __init__(self, config: Optional[StorageConfig] = None, v8_mode: bool = False):
        """ë©”ëª¨ë¦¬ ì €ì¥ ê´€ë¦¬ì ì´ˆê¸°í™”"""
        self.config = config or StorageConfig()
        self.base_path = Path(self.config.base_path) if isinstance(self.config.base_path, str) else self.config.base_path
        self.file_registry: Dict[str, FileInfo] = {}
        self._load_file_registry()
        self.v8_mode = v8_mode
        if v8_mode:
            try:
                self.v8_router = V8MemoryStoreRouter()
            except Exception as e:
                print(f"V8MemoryStoreRouter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.v8_router = None
        else:
            self.v8_router = None
        
        # LangChain VectorStore ìºì‹œ
        self._vectorstore = None  # ìƒì„± ì‹œ ìºì‹œí•´ë‘ê³  ì¬ì‚¬ìš©
    
    def _get_category_path(self, category: Optional[str]) -> Path:
        if not category:
            category = "unknown"
        return self.base_path / category
    
    def _get_subcategory_path(self, category: Optional[str], subcategory: Optional[str]) -> Path:
        if not category:
            category = "unknown"
        if not subcategory:
            subcategory = "general"
        return self.base_path / category / subcategory
    
    def _get_timestamped_filename(self, prefix: Optional[str], category: Optional[str] = "", subcategory: Optional[str] = "") -> str:
        if not prefix:
            prefix = "memory"
        if not category:
            category = "general"
        if not subcategory:
            subcategory = "data"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{prefix}_{timestamp}"
    
    def _get_compression_path(self, original_path: Optional[Path]) -> Path:
        if not original_path:
            return self.base_path / "compressed.jsonl.gz"
        return original_path.with_suffix('.jsonl.gz')
    
    def _get_decompression_path(self, compressed_path: Optional[Path]) -> Path:
        if not compressed_path:
            return self.base_path / "decompressed.jsonl"
        return compressed_path.with_suffix('.jsonl')
    
    def _load_jsonl_file(self, file_path: Optional[Path]) -> List[Dict[str, Any]]:
        if not file_path or not hasattr(file_path, 'exists') or not file_path.exists():
            return []
        try:
            if file_path.suffix == '.gz':
                with gzip.open(file_path, 'rt', encoding='utf-8') as f:
                    return [json.loads(line.strip()) for line in f if line.strip()]
            else:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return [json.loads(line.strip()) for line in f if line.strip()]
        except Exception as e:
            print(f"JSONL ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_json_file(self, file_path: Optional[Path]) -> List[Dict[str, Any]]:
        if not file_path or not hasattr(file_path, 'exists') or not file_path.exists():
            return []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return [data] if isinstance(data, dict) else data
        except Exception as e:
            print(f"JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def _load_file_registry(self):
        """ê¸°ì¡´ íŒŒì¼ ìŠ¤ìº”"""
        print("ğŸ” ê¸°ì¡´ ë©”ëª¨ë¦¬ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        
        # ê¸°ì¡´ íŒŒì¼ë“¤ì„ ì ì ˆí•œ ìœ„ì¹˜ë¡œ ì´ë™
        self._migrate_existing_files()
        
        # ëª¨ë“  íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
        for file_path in self._get_category_path("").rglob("*"):
            if file_path.is_file():
                self._register_file(file_path)
        
        print(f"âœ… {len(self.file_registry)}ê°œ íŒŒì¼ ë“±ë¡ ì™„ë£Œ")
    
    def _migrate_existing_files(self):
        """ê¸°ì¡´ íŒŒì¼ë“¤ì„ ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        migrations = [
            # Palantir íŒŒì¼ë“¤
            ("memory/data/palantir_graph.json", "palantir/main_graph.json"),
            
            # MemoryConductor íŒŒì¼ë“¤
            ("data/memory_data/entities.jsonl", "conductor/entities.jsonl"),
            ("data/memory_data/relationships.jsonl", "conductor/relationships.jsonl"),
            ("data/memory_data/contradictions.jsonl", "conductor/contradictions.jsonl"),
            ("data/memory_data/narrative_memories.jsonl", "conductor/narratives.jsonl"),
            
            # DataMemory íŒŒì¼ë“¤
            ("data/memory_data/h1.jsonl", "data_memory/h1.jsonl"),
            ("data/memory_data/h2.jsonl", "data_memory/h2.jsonl"),
            ("data/memory_data/h3.jsonl", "data_memory/h3.jsonl"),
            ("data/memory_data/ha1.jsonl", "data_memory/ha1.jsonl"),
            
            # ìºì‹œ íŒŒì¼ë“¤
            ("data/memory_data/thought_cache.jsonl", "cache/thought_cache.jsonl"),
            ("data/memory_data/memory_cache.jsonl", "cache/memory_cache.jsonl"),
            ("data/memory_data/loop_cache.jsonl", "cache/loop_cache.jsonl"),
            ("data/memory_data/emotional_cache.jsonl", "cache/emotional_cache.jsonl"),
            ("data/memory_data/cache_manager.json", "cache/cache_manager.json"),
            
            # ì˜ë„ ìºì‹œ íŒŒì¼ë“¤
            ("data/test_intent_cache/", "intent_cache/"),
        ]
        
        for source, destination in migrations:
            source_path = Path(source)
            dest_path = self._get_category_path(destination.split('/')[0]) / '/'.join(destination.split('/')[1:])
            
            if source_path.exists():
                try:
                    if source_path.is_dir():
                        # ë””ë ‰í† ë¦¬ ë³µì‚¬
                        if dest_path.exists():
                            shutil.rmtree(dest_path)
                        shutil.copytree(source_path, dest_path)
                    else:
                        # íŒŒì¼ ë³µì‚¬
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(source_path, dest_path)
                    
                    print(f"ğŸ“ {source} â†’ {destination}")
                except Exception as e:
                    print(f"âš ï¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ {source}: {e}")
    
    def _register_file(self, file_path: Path):
        """íŒŒì¼ ë“±ë¡"""
        try:
            stat = file_path.stat()
            file_info = FileInfo(
                path=file_path,
                size_bytes=stat.st_size,
                last_modified=datetime.fromtimestamp(stat.st_mtime),
                file_type=self._determine_file_type(file_path),
                memory_count=self._count_memories_in_file(file_path)
            )
            
            relative_path = str(file_path.relative_to(self._get_category_path("")))
            self.file_registry[relative_path] = file_info
            
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë“±ë¡ ì‹¤íŒ¨ {file_path}: {e}")
    
    def _determine_file_type(self, file_path: Path) -> str:
        """íŒŒì¼ íƒ€ì… ê²°ì •"""
        if file_path.suffix == '.json':
            return 'json'
        elif file_path.suffix == '.jsonl':
            return 'jsonl'
        elif file_path.suffix == '.gz':
            return 'compressed'
        else:
            return 'unknown'
    
    def _count_memories_in_file(self, file_path: Path) -> int:
        """íŒŒì¼ ë‚´ ë©”ëª¨ë¦¬ ê°œìˆ˜ ê³„ì‚°"""
        try:
            if file_path.suffix == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        # JSON ê°ì²´ì˜ ê²½ìš° í‚¤ ê°œìˆ˜ë‚˜ ë°°ì—´ ê¸¸ì´ë¡œ ì¶”ì •
                        if 'nodes' in data:
                            return len(data.get('nodes', []))
                        elif 'memories' in data:
                            return len(data.get('memories', []))
                        else:
                            return 1
                    elif isinstance(data, list):
                        return len(data)
                    else:
                        return 1
            elif file_path.suffix == '.jsonl':
                count = 0
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            count += 1
                return count
            else:
                return 0
        except Exception:
            return 0
    
    def save_memory(self, memory_data: Dict[str, Any], category: str, 
                   subcategory: str = None, filename: str = None, meta: Dict = None) -> str:
        """ë©”ëª¨ë¦¬ ì €ì¥ (V8 ì˜µì…˜ ì§€ì›)"""
        if self.v8_mode and self.v8_router is not None:
            # V8 êµ¬ì¡°ì˜ ë…¸ë“œ/ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì§€ì›
            return self.v8_router.store_with_condition(memory_data, meta or {})
        # ... ê¸°ì¡´ ì €ì¥ ë¡œì§ ...
        save_path = self._determine_save_path(category, subcategory, filename)
        return self._save_as_jsonl(memory_data, save_path)
    
    def _determine_save_path(self, category: str, subcategory: str = None, 
                           filename: str = None) -> Path:
        """ì €ì¥ ê²½ë¡œ ê²°ì •"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ê²½ë¡œ ë§¤í•‘ (v7 ê¸°ë³¸ + v8 í™•ì¥)
        category_paths = {
            # v7 ê¸°ì¡´ ê²½ë¡œ
            'integrated': 'integrated',
            'palantir': 'palantir',
            'conductor': 'conductor',
            'data_memory': 'data_memory',
            'cache': 'cache',
            'intent_cache': 'intent_cache',

            # v8 ì‹ ê·œ ë ˆì´ì–´ ê²½ë¡œ
            'hot': 'hot',      # ì‹¤ì‹œê°„ ì„¸ì…˜ ë²„í¼
            'warm': 'warm',    # ì„¸ì…˜ ë‹¨ìœ„ ë¬¸ë§¥ í’€
            'cold': 'cold',    # ì¥ê¸° ë³´ì¡´ ì•„ì¹´ì´ë¸Œ
            'graphs': 'graphs' # PalantirGraph ë° Universe ë¸Œëœì¹˜
        }
        
        base_category = category_paths.get(category, 'legacy')
        path_parts = [base_category]
        
        if subcategory:
            path_parts.append(subcategory)
        
        if filename:
            path_parts.append(filename)
        else:
            # ê¸°ë³¸ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            path_parts.append(f"{category}_{timestamp}")
        
        return self._get_category_path('/'.join(path_parts))
    
    def _determine_file_format(self, category: str) -> str:
        """íŒŒì¼ í˜•ì‹ ê²°ì •"""
        # ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ì €ì¥ í˜•ì‹ ì •ì˜
        format_map = {
            'integrated': 'jsonl',
            'palantir': 'json',
            'conductor': 'jsonl',
            'data_memory': 'jsonl',
            'cache': 'jsonl',
            'intent_cache': 'json',

            # v8 ë ˆì´ì–´ë³„ í˜•ì‹
            'hot': 'jsonl',   # ë‹¤ëŸ‰ append â†’ line ë‹¨ìœ„
            'warm': 'jsonl',  # í† í”½ ì„¸ì…˜ ê·¸ëŒ€ë¡œ append
            'cold': 'jsonl',  # ìš”ì•½Â·Narrative êµ¬ì¡°
            'graphs': 'json'  # PalantirGraph êµ¬ì¡°
        }
        
        return format_map.get(category, 'jsonl')
    
    def _save_as_jsonl(self, memory_data: Dict[str, Any], save_path: Path) -> str:
        if not save_path:
            return ""
        save_path = save_path.with_suffix('.jsonl')
        save_path.parent.mkdir(parents=True, exist_ok=True)
        should_compress = False
        if hasattr(self, 'config') and getattr(self.config, 'compression_enabled', False) and save_path.exists():
            if save_path.stat().st_size > 1024 * 1024:
                should_compress = True
                save_path = save_path.with_suffix('.jsonl.gz')
        try:
            if should_compress:
                with gzip.open(save_path, 'wt', encoding='utf-8') as f:
                    f.write(json.dumps(memory_data, ensure_ascii=False) + '\n')
            else:
                with open(save_path, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(memory_data, ensure_ascii=False) + '\n')
            self._register_file(save_path)
            return str(save_path)
        except Exception as e:
            print(f"JSONL ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def _save_as_json(self, memory_data: Dict[str, Any], save_path: Path) -> str:
        if not save_path:
            return ""
        save_path = save_path.with_suffix('.json')
        save_path.parent.mkdir(parents=True, exist_ok=True)
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(memory_data, f, ensure_ascii=False, indent=2)
            self._register_file(save_path)
            return str(save_path)
        except Exception as e:
            print(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def load_memory(self, file_path: str, memory_id: str = None) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
        """ë©”ëª¨ë¦¬ ë¡œë“œ"""
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        try:
            if path.suffix == '.json':
                return self._load_json_file(path)
            elif path.suffix == '.jsonl':
                return self._load_jsonl_file(path)
            elif path.suffix == '.gz':
                return self._load_compressed(path, memory_id)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {path.suffix}")
                
        except Exception as e:
            raise Exception(f"ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _load_compressed(self, path: Path, memory_id: str = None) -> List[Dict[str, Any]]:
        """ì••ì¶• íŒŒì¼ ë¡œë“œ"""
        memories = []
        
        with gzip.open(path, 'rt', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    memory = json.loads(line)
                    if memory_id is None or memory.get('id') == memory_id:
                        memories.append(memory)
                        if memory_id:  # íŠ¹ì • ID ì°¾ì•˜ìœ¼ë©´ ì¤‘ë‹¨
                            break
        
        return memories if memory_id is None else (memories[0] if memories else None)
    
    def search_memories(self, query: str, categories: List[str] = None, 
                       limit: int = 10) -> List[Dict[str, Any]]:
        """ë©”ëª¨ë¦¬ ê²€ìƒ‰"""
        results = []
        
        # ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ ê²°ì •
        if categories is None:
            categories = ['integrated', 'palantir', 'conductor', 'data_memory']
        
        for category in categories:
            category_results = self._search_in_category(query, category, limit // len(categories))
            results.extend(category_results)
        
        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        results.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        
        return results[:limit]
    
    def _search_in_category(self, query: str, category: str, limit: int) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ ë‚´ ê²€ìƒ‰"""
        results = []
        
        # ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        category_path = self._get_category_path(category)
        if not category_path.exists():
            return results
        
        # ì¹´í…Œê³ ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰
        for file_path in category_path.rglob('*'):
            if file_path.is_file() and file_path.suffix in ['.json', '.jsonl', '.gz']:
                try:
                    file_results = self._search_in_file(query, file_path, limit // 2)
                    results.extend(file_results)
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨ {file_path}: {e}")
        
        return results[:limit]
    
    def _search_in_file(self, query: str, file_path: Path, limit: int) -> List[Dict[str, Any]]:
        """íŒŒì¼ ë‚´ ê²€ìƒ‰"""
        results = []
        
        try:
            memories = self.load_memory(str(file_path))
            
            if isinstance(memories, list):
                for memory in memories:
                    relevance = self._calculate_relevance(query, memory)
                    if relevance > 0.3:  # ì„ê³„ê°’
                        memory['relevance'] = relevance
                        memory['source_file'] = str(file_path)
                        results.append(memory)
                        
                        if len(results) >= limit:
                            break
            elif isinstance(memories, dict):
                relevance = self._calculate_relevance(query, memories)
                if relevance > 0.3:
                    memories['relevance'] = relevance
                    memories['source_file'] = str(file_path)
                    results.append(memories)
                    
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ê²€ìƒ‰ ì˜¤ë¥˜ {file_path}: {e}")
        
        return results
    
    def _calculate_relevance(self, query: str, memory: Dict[str, Any]) -> float:
        """ê´€ë ¨ì„± ê³„ì‚°"""
        relevance = 0.0
        query_lower = query.lower()
        
        # ë‚´ìš© ë§¤ì¹­
        content = memory.get('content', '').lower()
        if query_lower in content:
            relevance += 0.5
        
        # íƒœê·¸ ë§¤ì¹­
        tags = memory.get('tags', [])
        for tag in tags:
            if query_lower in tag.lower():
                relevance += 0.3
        
        # ì œëª© ë§¤ì¹­
        title = memory.get('title', '').lower()
        if query_lower in title:
            relevance += 0.4
        
        return min(1.0, relevance)
    
    def create_backup(self, backup_type: str = 'daily') -> str:
        """ë°±ì—… ìƒì„±"""
        if not self.config.backup_enabled:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{backup_type}_{timestamp}"
        backup_path = self._get_category_path("backups") / backup_type / backup_name
        
        try:
            # ì „ì²´ ë©”ëª¨ë¦¬ ë””ë ‰í† ë¦¬ ë°±ì—…
            shutil.copytree(self._get_category_path("integrated"), backup_path / "integrated")
            shutil.copytree(self._get_category_path("palantir"), backup_path / "palantir")
            shutil.copytree(self._get_category_path("conductor"), backup_path / "conductor")
            shutil.copytree(self._get_category_path("data_memory"), backup_path / "data_memory")
            shutil.copytree(self._get_category_path("cache"), backup_path / "cache")
            
            # ë°±ì—… ë©”íƒ€ë°ì´í„° ìƒì„±
            backup_meta = {
                "backup_type": backup_type,
                "created_at": datetime.now().isoformat(),
                "file_count": len(self.file_registry),
                "total_size": sum(fi.size_bytes for fi in self.file_registry.values())
            }
            
            with open(backup_path / "backup_meta.json", 'w', encoding='utf-8') as f:
                json.dump(backup_meta, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_name}")
            return str(backup_path)
            
        except Exception as e:
            print(f"âš ï¸ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def cleanup_old_files(self):
        """ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=self.config.auto_cleanup_days)
        
        files_to_remove = []
        for relative_path, file_info in self.file_registry.items():
            if file_info.last_modified < cutoff_date:
                files_to_remove.append(relative_path)
        
        for relative_path in files_to_remove:
            file_path = self._get_category_path(relative_path)
            try:
                file_path.unlink()
                del self.file_registry[relative_path]
                print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {relative_path}")
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {relative_path}: {e}")
    
    def cleanup_old_backups(self):
        """ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        if not self.config.backup_enabled:
            return
        
        cutoff_date = datetime.now() - timedelta(days=self.config.backup_retention_days)
        backup_base = self._get_category_path("backups")
        
        for backup_type in ['daily', 'weekly', 'monthly']:
            backup_dir = backup_base / backup_type
            if not backup_dir.exists():
                continue
            
            for backup_path in backup_dir.iterdir():
                if backup_path.is_dir():
                    # ë°±ì—… ë©”íƒ€ë°ì´í„° í™•ì¸
                    meta_file = backup_path / "backup_meta.json"
                    if meta_file.exists():
                        try:
                            with open(meta_file, 'r', encoding='utf-8') as f:
                                meta = json.load(f)
                                created_at = datetime.fromisoformat(meta['created_at'])
                                
                                if created_at < cutoff_date:
                                    shutil.rmtree(backup_path)
                                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {backup_path.name}")
                        except Exception as e:
                            print(f"âš ï¸ ë°±ì—… ë©”íƒ€ë°ì´í„° ì½ê¸° ì‹¤íŒ¨ {meta_file}: {e}")
    
    def get_storage_stats(self) -> Dict[str, Any]:
        """ì €ì¥ì†Œ í†µê³„"""
        total_size = sum(fi.size_bytes for fi in self.file_registry.values())
        total_memories = sum(fi.memory_count for fi in self.file_registry.values())
        
        stats = {
            "total_files": len(self.file_registry),
            "total_size_bytes": total_size,
            "total_size_mb": total_size / (1024 * 1024),
            "total_memories": total_memories,
            "categories": {},
            "file_types": {},
            "largest_files": [],
            "recent_files": []
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for relative_path, file_info in self.file_registry.items():
            category = relative_path.split('/')[0]
            if category not in stats["categories"]:
                stats["categories"][category] = {
                    "file_count": 0,
                    "total_size": 0,
                    "memory_count": 0
                }
            
            stats["categories"][category]["file_count"] += 1
            stats["categories"][category]["total_size"] += file_info.size_bytes
            stats["categories"][category]["memory_count"] += file_info.memory_count
        
        # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
        for file_info in self.file_registry.values():
            file_type = file_info.file_type
            if file_type not in stats["file_types"]:
                stats["file_types"][file_type] = 0
            stats["file_types"][file_type] += 1
        
        # ê°€ì¥ í° íŒŒì¼ë“¤
        largest_files = sorted(self.file_registry.items(), 
                             key=lambda x: x[1].size_bytes, reverse=True)[:10]
        stats["largest_files"] = [
            {"path": path, "size_mb": fi.size_bytes / (1024 * 1024)}
            for path, fi in largest_files
        ]
        
        # ìµœê·¼ íŒŒì¼ë“¤
        recent_files = sorted(self.file_registry.items(), 
                            key=lambda x: x[1].last_modified, reverse=True)[:10]
        stats["recent_files"] = [
            {"path": path, "modified": fi.last_modified.isoformat()}
            for path, fi in recent_files
        ]
        
        return stats
    
    def optimize_storage(self):
        """ì €ì¥ì†Œ ìµœì í™”"""
        print("ğŸ”§ ì €ì¥ì†Œ ìµœì í™” ì‹œì‘...")
        
        # 1. ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
        self.cleanup_old_files()
        
        # 2. ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
        self.cleanup_old_backups()
        
        # 3. í° íŒŒì¼ ì••ì¶•
        self._compress_large_files()
        
        # 4. ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬
        self._deduplicate_files()
        
        # 5. íŒŒì¼ ì •ë³´ ì¬ìŠ¤ìº”
        self._rescan_files()
        
        print("âœ… ì €ì¥ì†Œ ìµœì í™” ì™„ë£Œ")
    
    def _compress_large_files(self):
        """í° íŒŒì¼ ì••ì¶•"""
        for relative_path, file_info in self.file_registry.items():
            if (file_info.size_bytes > self.config.max_file_size_mb * 1024 * 1024 and 
                file_info.file_type in ['json', 'jsonl']):
                
                file_path = self._get_category_path(relative_path)
                compressed_path = self._get_compression_path(file_path)
                
                try:
                    with open(file_path, 'rb') as f_in:
                        with gzip.open(compressed_path, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                    
                    # ì›ë³¸ íŒŒì¼ ì‚­ì œ
                    file_path.unlink()
                    
                    # íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸
                    self._register_file(compressed_path)
                    del self.file_registry[relative_path]
                    
                    print(f"ğŸ—œï¸ íŒŒì¼ ì••ì¶•: {relative_path}")
                    
                except Exception as e:
                    print(f"âš ï¸ íŒŒì¼ ì••ì¶• ì‹¤íŒ¨ {relative_path}: {e}")
    
    def _deduplicate_files(self):
        """ì¤‘ë³µ íŒŒì¼ ê²€ì‚¬ ë° ì •ë¦¬"""
        # ê°„ë‹¨í•œ ì¤‘ë³µ ê²€ì‚¬ (íŒŒì¼ í¬ê¸°ì™€ ë‚´ìš© ê¸°ë°˜)
        file_hashes = {}
        
        for relative_path, file_info in self.file_registry.items():
            file_path = self._get_category_path(relative_path)
            
            try:
                # ê°„ë‹¨í•œ í•´ì‹œ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í•´ì‹œ ì‚¬ìš©)
                with open(file_path, 'rb') as f:
                    content = f.read()
                    file_hash = hash(content)
                
                if file_hash in file_hashes:
                    # ì¤‘ë³µ íŒŒì¼ ë°œê²¬
                    original_path = file_hashes[file_hash]
                    print(f"ğŸ”„ ì¤‘ë³µ íŒŒì¼ ë°œê²¬: {relative_path} â†’ {original_path}")
                    
                    # ì¤‘ë³µ íŒŒì¼ ì‚­ì œ
                    file_path.unlink()
                    del self.file_registry[relative_path]
                else:
                    file_hashes[file_hash] = relative_path
                    
            except Exception as e:
                print(f"âš ï¸ ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨ {relative_path}: {e}")
    
    def _rescan_files(self):
        """íŒŒì¼ ì •ë³´ ì¬ìŠ¤ìº”"""
        self.file_registry.clear()
        for file_path in self._get_category_path("").rglob("*"):
            if file_path.is_file():
                self._register_file(file_path)

    def some_function_returning_str(self, value: Optional[str]) -> str:
        if value is None:
            return ""
        return value

    def some_function_returning_list(self, value: Optional[list]) -> list:
        if value is None:
            return []
        return value

    # ì•„ë˜ì™€ ê°™ì´ str íŒŒë¼ë¯¸í„°ì— Noneì´ ë“¤ì–´ì˜¬ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë“¤ ëª¨ë‘ Optional[str]ë¡œ ë³€ê²½
    def example_func(self, arg1: Optional[str], arg2: Optional[str] = None) -> str:
        if not arg1:
            return ""
        if not arg2:
            arg2 = "default"
        return f"{arg1}-{arg2}"

    # ë°˜í™˜ íƒ€ì…ì´ strì¸ë° Noneì´ ë°˜í™˜ë  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ëŠ” ëª¨ë‘ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    def another_func(self, arg: Optional[str]) -> str:
        if not arg:
            return ""
        return arg

    # ë°˜í™˜ íƒ€ì…ì´ List[str]ì¸ë° Noneì´ ë°˜í™˜ë  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ëŠ” ëª¨ë‘ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    def another_list_func(self, arg: Optional[list]) -> list:
        if not arg:
            return []
        return arg

    def register_existing_files(self) -> int:
        """ê¸°ì¡´ íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•˜ì—¬ file_registryì— ë“±ë¡"""
        registered_count = 0
        
        try:
            # base_pathê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if not self.base_path.exists():
                print(f"âš ï¸ ê¸°ë³¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {self.base_path}")
                return 0
            
            # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”
            for file_path in self.base_path.rglob("*"):
                if file_path.is_file() and file_path.suffix in ['.json', '.jsonl', '.jsonl.gz']:
                    try:
                        # íŒŒì¼ ì •ë³´ ìƒì„±
                        file_info = FileInfo(
                            path=str(file_path),
                            size_bytes=file_path.stat().st_size,
                            last_modified=datetime.fromtimestamp(file_path.stat().st_mtime),
                            file_type=self._determine_file_type(file_path),
                            memory_count=self._count_memories_in_file(file_path)
                        )
                        
                        # file_registryì— ë“±ë¡
                        self.file_registry[str(file_path)] = file_info
                        registered_count += 1
                        
                    except Exception as e:
                        print(f"âš ï¸ íŒŒì¼ ë“±ë¡ ì‹¤íŒ¨ {file_path}: {e}")
            
            # file_registry ì €ì¥
            self._save_file_registry()
            
            print(f"âœ… {registered_count}ê°œ íŒŒì¼ ë“±ë¡ ì™„ë£Œ")
            return registered_count
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë“±ë¡ ì¤‘ ì˜¤ë¥˜: {e}")
            return registered_count
    
    def _extract_category_from_path(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        try:
            # base_pathë¥¼ ì œì™¸í•œ ìƒëŒ€ ê²½ë¡œì—ì„œ ì²« ë²ˆì§¸ ë””ë ‰í† ë¦¬ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ì‚¬ìš©
            relative_path = file_path.relative_to(self.base_path)
            parts = relative_path.parts
            
            if len(parts) > 0:
                return parts[0]
            return "unknown"
        except:
            return "unknown"
    
    def _extract_subcategory_from_path(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ì„œë¸Œì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        try:
            # base_pathë¥¼ ì œì™¸í•œ ìƒëŒ€ ê²½ë¡œì—ì„œ ë‘ ë²ˆì§¸ ë””ë ‰í† ë¦¬ë¥¼ ì„œë¸Œì¹´í…Œê³ ë¦¬ë¡œ ì‚¬ìš©
            relative_path = file_path.relative_to(self.base_path)
            parts = relative_path.parts
            
            if len(parts) > 1:
                return parts[1]
            return "general"
        except:
            return "general"

    # --------------------
    # V8 DATA-MEMORY HELPER
    # --------------------
    def _data_memory_paths(self) -> List[Path]:
        """h, h1, h2, h3, ha1 ë“± ë°ì´í„° ë©”ëª¨ë¦¬ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        candidates = [
            self.base_path / "data_memory" / fname for fname in (
                "h.jsonl", "h1.jsonl", "h2.jsonl", "h3.jsonl", "ha1.jsonl"
            )
        ]
        return [p for p in candidates if p.exists()]

    def _load_data_memory_entries(self) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë©”ëª¨ë¦¬(h, h1~3, ha1) ëª¨ë“  ë…¸ë“œ ë¡œë“œ"""
        entries: List[Dict[str, Any]] = []
        for path in self._data_memory_paths():
            try:
                entries.extend(self._load_jsonl_file(path))
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨ {path}: {e}")
        return entries

    # --- ìœ ì‚¬ë„ ê³„ì‚° ìœ í‹¸ ---
    def _tokenize(self, text: str) -> set:
        import re
        return set(re.findall(r"[\wê°€-í£]+", text.lower()))

    def _jaccard_similarity(self, a: str, b: str) -> float:
        """ê°„ë‹¨í•œ Jaccard ìœ ì‚¬ë„ (í† í° ê¸°ë°˜)"""
        set_a, set_b = self._tokenize(a), self._tokenize(b)
        if not set_a or not set_b:
            return 0.0
        intersection = len(set_a & set_b)
        union = len(set_a | set_b)
        return intersection / union if union else 0.0

    # --- ê³µê°œ API ---
    def find_similar_memories(self, query: str, top_k: int = 5, threshold: float = 0.3) -> List[Dict[str, Any]]:
        """query í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬í•œ h* ë©”ëª¨ë¦¬ ë…¸ë“œë¥¼ top_k ê°œ ë°˜í™˜"""
        entries = self._load_data_memory_entries()
        scored: List[Tuple[float, Dict[str, Any]]] = []
        for entry in entries:
            content = entry.get("content") or ""
            score = self._jaccard_similarity(query, content)
            if score >= threshold:
                scored.append((score, entry))
        scored.sort(key=lambda x: x[0], reverse=True)
        return [e for _, e in scored[:top_k]]

    def build_similarity_map(self, threshold: float = 0.4) -> Dict[str, List[str]]:
        """h* ë©”ëª¨ë¦¬ë“¤ ê°„ ìœ ì‚¬ë„ â‰¥ threshold ì¸ ë…¸ë“œ ë§¤í•‘(id -> [similar_ids])"""
        entries = self._load_data_memory_entries()
        id_to_content = {e.get("id"): e.get("content", "") for e in entries if e.get("id")}
        ids = list(id_to_content.keys())
        mapping: Dict[str, List[str]] = {}
        for i, id_a in enumerate(ids):
            content_a = id_to_content[id_a]
            sims: List[str] = []
            for id_b in ids[i+1:]:
                content_b = id_to_content[id_b]
                if self._jaccard_similarity(content_a, content_b) >= threshold:
                    sims.append(id_b)
            if sims:
                mapping[id_a] = sims
        return mapping

    # ==================================================
    # LangChain VectorStore & LangGraph Tool integration
    # ==================================================

    # --- LangChain Helper ---
    def _to_document(self, entry: Dict[str, Any]):
        """h* entry â†’ LangChain Document ë³€í™˜ (LangChain ì„¤ì¹˜ ì‹œ)"""
        if not LANGCHAIN_AVAILABLE:
            return None
        metadata = {k: entry.get(k) for k in ("id", "type", "tags", "context")}
        return Document(page_content=entry.get("content", ""), metadata=metadata)

    def build_vectorstore(self, force_rebuild: bool = False):
        """FAISS VectorStoreë¥¼ ìƒì„±/ë°˜í™˜ (LangChain í•„ìš”)"""
        if not LANGCHAIN_AVAILABLE:
            print("âš ï¸ LangChain ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ VectorStoreë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        if self._vectorstore is not None and not force_rebuild:
            return self._vectorstore

        docs = [self._to_document(e) for e in self._load_data_memory_entries()]
        docs = [d for d in docs if d is not None]
        if not docs:
            print("âš ï¸ ë²¡í„°ìŠ¤í† ì–´ë¡œ ë³€í™˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        embeddings = OpenAIEmbeddings()
        self._vectorstore = FAISS.from_documents(docs, embeddings)
        return self._vectorstore

    def query_vectorstore(self, query: str, k: int = 5):
        """VectorStore ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        vs = self.build_vectorstore()
        if vs is None:
            return []
        try:
            return vs.similarity_search(query, k=k)
        except Exception as e:
            print(f"âš ï¸ VectorStore ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    # --- LangGraph Helper ---
    def _find_entry_by_id(self, entry_id: str) -> Optional[Dict[str, Any]]:
        for entry in self._load_data_memory_entries():
            if entry.get("id") == entry_id:
                return entry
        return None

    def _entry_to_toolnode(self, entry: Dict[str, Any]):
        """h* entryë¥¼ LangGraph ToolNode ë¡œ ë³€í™˜"""
        if not LANGGRAPH_AVAILABLE or entry is None:
            return None

        def _tool_fn(state):  # LangGraph state â†’ dict ë°˜í™˜
            return {
                "memory": entry.get("content", ""),
                "tags": entry.get("tags", [])
            }

        return ToolNode(_tool_fn, name=f"memory_tool_{entry.get('id', '')[:6]}")

    def export_memory_to_langgraph(self, graph: "StateGraph", entry_ids: List[str]):  # type: ignore
        """ì§€ì • id ë…¸ë“œë“¤ì„ LangGraph ê·¸ë˜í”„ì— ToolNode ë¡œ ì‚½ì…"""
        if not LANGGRAPH_AVAILABLE:
            print("âš ï¸ LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê·¸ë˜í”„ì— ë©”ëª¨ë¦¬ë¥¼ ì‚½ì…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return graph

        for eid in entry_ids:
            entry = self._find_entry_by_id(eid)
            tool_node = self._entry_to_toolnode(entry)
            if tool_node:
                try:
                    graph.add_node(tool_node)
                    graph.add_edge(tool_node, END)  # type: ignore
                except Exception as e:
                    print(f"âš ï¸ ToolNode ì‚½ì… ì‹¤íŒ¨({eid}): {e}")

        # ë©”ëª¨ë¦¬ ì„¸ì´ë²„ë¡œ ì²´í¬í¬ì¸íŠ¸ í™œì„±í™”
        try:
            MemorySaver(graph)
        except Exception as e:
            print(f"âš ï¸ MemorySaver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return graph


# ì‚¬ìš© ì˜ˆì‹œ
def create_memory_storage_manager(config: StorageConfig = None) -> MemoryStorageManager:
    """ë©”ëª¨ë¦¬ ì €ì¥ ê´€ë¦¬ì ìƒì„±"""
    return MemoryStorageManager(config)


# ê¸°ë³¸ ì„¤ì •
DEFAULT_STORAGE_CONFIG = StorageConfig(
    base_path="memory/data",
    backup_enabled=True,
    compression_enabled=True,
    max_file_size_mb=50,
    auto_cleanup_days=30,
    backup_retention_days=90
)

# === V8 ì €ì¥/ë¼ìš°íŒ…/í†µê³„ êµ¬ì¡° í†µí•© ===
from pathlib import Path as SysPath
from typing import Optional as TypingOptional, Dict as TypingDict
from datetime import datetime as dt_datetime
import json as sys_json
import os as sys_os

class V8MemoryStoreRouter:
    def __init__(self):
        self.paths = {
            "cold": {
                "h1": SysPath("data/memory_data/h1.jsonl"),
                "h2": SysPath("data/memory_data/h2.jsonl"),
                "h3": SysPath("data/memory_data/h3.jsonl"),
            },
            "hot": SysPath("data/memory_data/ha1.jsonl"),
            "warm": {
                "emotional": SysPath("data/memory_data/emotional_cache.jsonl"),
                "loop": SysPath("data/memory_data/loop_cache.jsonl"),
                "general": SysPath("data/memory_data/memory_cache.jsonl")
            }
        }
        self.stats = {
            "cold": 0,
            "hot": 0,
            "warm": 0,
            "errors": 0
        }
        sys_os.makedirs("data/memory_data", exist_ok=True)

    def store_with_condition(self, node, meta: TypingDict) -> TypingOptional[str]:
        try:
            if meta.get("user_acknowledged") is True:
                h_level = meta.get("target_h_level", "h1")
                path = self.paths["cold"].get(h_level, self.paths["cold"]["h1"])
                self.stats["cold"] += 1
                return self._append_line(path, node)

            elif not getattr(node, 'scar_flagged', False) and getattr(node, 'confidence', 0.0) >= 0.7:
                self.stats["hot"] += 1
                return self._append_line(self.paths["hot"], node)

            else:
                if getattr(node, 'emotion_vector', None) and sum(getattr(node, 'emotion_vector', [])) > 0.5:
                    self.stats["warm"] += 1
                    return self._append_line(self.paths["warm"]["emotional"], node)
                elif hasattr(node, 'tags') and "loop" in getattr(node, 'tags', []):
                    self.stats["warm"] += 1
                    return self._append_line(self.paths["warm"]["loop"], node)
                else:
                    self.stats["warm"] += 1
                    return self._append_line(self.paths["warm"]["general"], node)

        except Exception as e:
            self.stats["errors"] += 1
            self._log_error(node, e)
            return None

    def _append_line(self, path: SysPath, node) -> str:
        # nodeëŠ” dict ë˜ëŠ” dataclass ëª¨ë‘ ì§€ì›
        if hasattr(node, '__dict__'):
            line = sys_json.dumps(node.__dict__, ensure_ascii=False, default=str)
        else:
            line = sys_json.dumps(node, ensure_ascii=False, default=str)
        with open(path, 'a', encoding='utf-8') as f:
            f.write(line + "\n")
        return str(path)

    def _log_error(self, node, error: Exception):
        log_path = SysPath("data/memory_data/store_errors.log")
        with open(log_path, 'a', encoding='utf-8') as f:
            f.write(f"[{dt_datetime.utcnow()}] Error storing {getattr(node, 'node_id', 'unknown')}: {str(error)}\n")

    def report_stats(self) -> TypingDict:
        return self.stats 